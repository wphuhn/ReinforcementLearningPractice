{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROM = \"Breakout-v0\"\n",
    "MODE = \"start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "output_folder = \"./\" + ROM\n",
    "pathlib.Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for OpenAI Gym Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import State, Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get game info\n",
    "game = Game(rom=ROM).create()\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = deque()\n",
    "game.create()\n",
    "action = game.sample()\n",
    "state, _, _, _ = game.step_state(action)\n",
    "state.plot()\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following architecture was taken from the Atari paper, except I downsampled to 168 x 168."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import AtariModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "    net = AtariModel()\n",
    "    net.half()\n",
    "    net.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    opt = optim.RMSprop(params=net.parameters(), lr=0.00025, momentum=0.95, eps=0.01)\n",
    "    return net, criterion, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepopulate Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer().populate(game, n_states=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "frame_rate = 1./60.\n",
    "for i in range(12500):\n",
    "    state = replay_buffer.replay[i][0].float().numpy()\n",
    "    for j in range(4):\n",
    "        frame = state[j]\n",
    "        plt.imshow(frame, cmap=plt.cm.binary)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        print(f\"i {i} j {j}\")\n",
    "        time.sleep(frame_rate)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning import QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training(output_folder, device, load_replay=False, suffix=\"\"):\n",
    "    net, criterion, optimizer = create_model(device)\n",
    "\n",
    "    with open(f\"{output_folder}/training{suffix}.pkl\", \"rb\") as training_file:\n",
    "        training = pkl.load(training_file)\n",
    "    with open(f\"{output_folder}/game{suffix}.pkl\", \"rb\") as game_file:\n",
    "        game = pkl.load(game_file)\n",
    "\n",
    "    with open(f\"{output_folder}/net{suffix}.pth\", \"rb\") as net_file:\n",
    "        net.load_state_dict(torch.load(net_file))\n",
    "        net.to(device)\n",
    "    with open(f\"{output_folder}/criterion{suffix}.pth\", \"rb\") as criterion_file:\n",
    "        criterion.load_state_dict(torch.load(criterion_file))\n",
    "    with open(f\"{output_folder}/optimizer{suffix}.pth\", \"rb\") as optimizer_file:\n",
    "        optimizer.load_state_dict(torch.load(optimizer_file))\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    replay = None\n",
    "    if load_replay:\n",
    "        with open(f\"{output_folder}/replay{suffix}.pkl\", \"rb\") as replay_file:\n",
    "            replay = pkl.load(replay_file)\n",
    "                    \n",
    "    return training, game, net, criterion, optimizer, replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "game.create()\n",
    "if MODE == \"start\":\n",
    "    net, criterion, optimizer = create_model(device)\n",
    "    replay = None\n",
    "    training = QLearning(output_folder=output_folder)\n",
    "elif MODE == \"load_from_disk\":\n",
    "    training, game, net, criterion, optimizer, replay = load_training(output_folder, device=device, suffix=\".latest\")\n",
    "losses, rewards_episode, replay = training.train(game, net, criterion, optimizer, replay=replay, device=device)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from time import time\n",
    "from gym import wrappers\n",
    "\n",
    "def play_game(game, net, device, output_folder, n_steps=500, frame_rate=1./15.):\n",
    "    game.create()\n",
    "    _ = game.reset()\n",
    "    \n",
    "    # Pick a random action initially\n",
    "    print(\"1\")\n",
    "    action = game.sample()\n",
    "    state, _, _, _ = game.step_state(action)\n",
    "    reward_game = 0\n",
    "\n",
    "    while True:\n",
    "        print(\"2\")\n",
    "        q_values = net(torch.tensor(state.frames, dtype=torch.half, device=device).unsqueeze(0))\n",
    "        # Needs to reside on CPU to be fed to OpenAI Gym, and argmax doesn't accept half precision\n",
    "        with torch.no_grad():\n",
    "            q_values = net(torch.tensor(state.frames, dtype=torch.half, device=device).unsqueeze(0))\n",
    "            q_values = q_values.clone().detach().float().cpu()\n",
    "            action = int(torch.argmax(q_values).data.numpy())\n",
    "\n",
    "        print(\"3\")\n",
    "        next_state, reward, done, info = game.step_state(action)\n",
    "        game.env.render()\n",
    "        sleep(frame_rate)\n",
    "\n",
    "        print(\"4\")\n",
    "        reward_game += reward\n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        print(\"5\")\n",
    "        print(f\"Reward: {reward_game}\")\n",
    "        print(f\"Action: {action}\")\n",
    "        state = next_state\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, net, _, _, _ = load_training(output_folder, device, suffix=\".latest\")\n",
    "game = Game()\n",
    "play_game(game, net, device, \"videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"videos/1588787664.302215/openaigym.video.1.7715.video000000.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
