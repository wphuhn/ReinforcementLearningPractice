{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROM_NAME = \"Breakout-v0\"\n",
    "N_OUTPUTS = 4 # Available via game.n_actions\n",
    "MODE = \"start\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "output_folder = \"./\" + ROM_NAME\n",
    "pathlib.Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for OpenAI Gym Retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, n_frames=4, frame_size=(84, 84)):\n",
    "        self.n_frames = n_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.frames = None\n",
    "        \n",
    "    def create(self, buffer):\n",
    "        # The buffer has n_frames+1 frames, as it performs smoothing\n",
    "\n",
    "        # Each pseudo-frame in the state is the maximum of the channel values\n",
    "        # between a frame and the previous frame in the buffer (this is done to\n",
    "        # handle flickering)\n",
    "        rolling_maxs = []\n",
    "        for i in range(self.n_frames):\n",
    "            rolling_maxs.append(np.max([buffer[i+1], buffer[i]], axis=0))\n",
    "        '''\n",
    "        rolling_maxs.append(np.max([oldest, buffer[0]], axis=0))    \n",
    "        for i, frame in enumerate(buffer):\n",
    "            if i > 0:\n",
    "                rolling_maxs.append(np.max([buffer[i-1], frame], axis=0))\n",
    "        '''\n",
    "        \n",
    "        # Convert RGB to luminance\n",
    "        # Note: I am assuming that the RBG values output by OpenAI Gym are\n",
    "        # already linear\n",
    "        lums = []\n",
    "        for frame in rolling_maxs:\n",
    "            lums.append(0.2126 * frame[:, :, 0] + 0.7152 * frame[:, :, 1] + 0.0722 * frame[:, :, 2])\n",
    "\n",
    "        # Normalize and resize frames to target size\n",
    "        self.frames = []\n",
    "        for frame in lums:\n",
    "            frame /= 255.\n",
    "            self.frames.append(cv2.resize(frame, dsize=self.frame_size))\n",
    "\n",
    "        self.frames = np.array(self.frames)\n",
    "        return self\n",
    "\n",
    "    def plot(self):\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(20, 20))\n",
    "        axs[0, 0].imshow(self.frames[3], cmap=plt.cm.binary)\n",
    "        axs[0, 0].set_title(\"t\")\n",
    "        axs[0, 1].imshow(self.frames[2], cmap=plt.cm.binary)\n",
    "        axs[0, 1].set_title(\"t-1\")\n",
    "        axs[1, 0].imshow(self.frames[1], cmap=plt.cm.binary)\n",
    "        axs[1, 0].set_title(\"t-2\")\n",
    "        axs[1, 1].imshow(self.frames[0], cmap=plt.cm.binary)\n",
    "        axs[1, 1].set_title(\"t-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "    def __init__(self, name=ROM_NAME):\n",
    "        self.env = None\n",
    "        self.frame = None\n",
    "        self.n_actions = None\n",
    "        self.name = name\n",
    "    \n",
    "    def create(self):\n",
    "        if self.env is not None:\n",
    "            self.close()\n",
    "        self.env = gym.envs.make(self.name)\n",
    "        self.frame = self.env.reset()\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        return self\n",
    "    \n",
    "    def get_action_meanings(self):\n",
    "        return self.env.unwrapped.get_action_meanings()\n",
    "    \n",
    "    def sample(self):\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def step(self, action):\n",
    "        frame, reward, done, info = self.env.step(action)\n",
    "        self.frame = frame\n",
    "        return frame, reward, done, info\n",
    "\n",
    "    def step_state(self, action, n_frames=4, frame_size=(84, 84)):\n",
    "        buffer = deque()\n",
    "        buffer.append(self.frame)\n",
    "        \n",
    "        reward = 0\n",
    "        for i in range(n_frames):\n",
    "            frame, r, done, info = self.step(action)\n",
    "            reward += r\n",
    "            buffer.append(frame)\n",
    "        return State(n_frames, frame_size).create(buffer), reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        self.frame = frame\n",
    "        return frame\n",
    "\n",
    "    def close(self):\n",
    "        self.env.render()\n",
    "        self.env.close()\n",
    "        self.env = None\n",
    "        self.frame = None\n",
    "        self.n_actions = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get game info\n",
    "game = Game().create()\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = deque()\n",
    "game.create()\n",
    "action = game.sample()\n",
    "state, _, _, _ = game.step_state(action)\n",
    "state.plot()\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following architecture was taken from the Atari paper, except I downsampled to 168 x 168."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FRAMES = 4\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(N_FRAMES, 32, 8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, 3, stride=1)\n",
    "        self.linear_1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.linear_2 = nn.Linear(512, N_OUTPUTS)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv_1(x))\n",
    "        y = F.relu(self.conv_2(y))\n",
    "        y = F.relu(self.conv_3(y))\n",
    "        y = y.view(y.size(0), -1)\n",
    "        y = F.relu(self.linear_1(y))\n",
    "        return self.linear_2(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "    net = model()\n",
    "    net.half()\n",
    "    net.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    opt = optim.RMSprop(params=net.parameters(), lr=0.00025, momentum=0.95, eps=0.01)\n",
    "    return net, criterion, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepopulate Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    # DeepMind 2015 used last 1 million frames, which corresponds to 250,000 states for 4-frame states\n",
    "    def __init__(self, max_len=250000):\n",
    "        self.replay = None\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Takes in a collection of Python/numpy primitives and converts them to Tensors\n",
    "        # before appending to the replay buffer\n",
    "        state_tensor = torch.tensor(state.frames, dtype=torch.half)\n",
    "        action_tensor = torch.tensor([action], dtype=torch.uint8)\n",
    "        reward_tensor = torch.tensor([reward], dtype=torch.half)\n",
    "        next_state_tensor = torch.tensor(next_state.frames, dtype=torch.half)\n",
    "        done_tensor = torch.tensor([done], dtype=torch.uint8)\n",
    "\n",
    "        self.replay.append((state_tensor, action_tensor, reward_tensor, next_state_tensor, done_tensor))\n",
    "    \n",
    "    def populate(self, game, n_states=12500, n_frames_per_state=4):\n",
    "        self.replay = deque(maxlen=self.max_len)\n",
    "        game.create()\n",
    "        \n",
    "        # Create initial state\n",
    "        action = game.sample()\n",
    "        state, _, _, _ = game.step_state(action)\n",
    "    \n",
    "        for step in range(n_states):\n",
    "            action = game.sample()\n",
    "            next_state, reward, done, _ = game.step_state(action)\n",
    "\n",
    "            self.add(state, action, reward, next_state, done)\n",
    "\n",
    "            if step % 1000 == 1:\n",
    "                print(f\"On step {step} of replay buffer\")\n",
    "\n",
    "            if done:\n",
    "                _ = game.reset()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def create_mini_batch(self, batch_size=32):\n",
    "        # Generate mini-batch\n",
    "        mini_batch = random.sample(self.replay, batch_size)\n",
    "        \n",
    "        tensors = dict()\n",
    "        tensors[\"action\"] = torch.stack([a for (s, a, r, s_n, d) in mini_batch]).squeeze()\n",
    "        tensors[\"reward\"] = torch.stack([r for (s, a, r, s_n, d) in mini_batch]).squeeze()\n",
    "        tensors[\"done\"] = torch.stack([d for (s, a, r, s_n, d) in mini_batch]).squeeze()\n",
    "        tensors[\"state\"] = torch.stack([s for (s, a, r, s_n, d) in mini_batch])\n",
    "        tensors[\"next_state\"] = torch.stack([s_n for (s, a, r, s_n, d) in mini_batch])        \n",
    "\n",
    "        return tensors\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.replay[key]\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.replay[key] = value\n",
    "        return self.replay[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer().populate(game, n_states=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "frame_rate = 1./60.\n",
    "for i in range(12500):\n",
    "    state = replay_buffer.replay[i][0].float().numpy()\n",
    "    for j in range(4):\n",
    "        frame = state[j]\n",
    "        plt.imshow(frame, cmap=plt.cm.binary)\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        print(f\"i {i} j {j}\")\n",
    "        time.sleep(frame_rate)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from time import time\n",
    "import pickle as pkl\n",
    "\n",
    "class QLearning(object):\n",
    "    def __init__(self, output_folder=None, max_steps_per_eps=50000, n_steps=50000000, gamma=0.99, n_frames_per_state=4):\n",
    "        self.output_folder = output_folder\n",
    "        self.max_steps_per_eps = max_steps_per_eps\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.n_frames_per_state=4\n",
    "        \n",
    "        self.timestep = 0\n",
    "        self.epoch = 0\n",
    "        self.losses = []\n",
    "        self.rewards_episode = []\n",
    "\n",
    "    def save(self, game, net, criterion, optimizer, replay, save_replay=False, suffix=\"\"):\n",
    "        with open(f\"{output_folder}/training{suffix}.pkl\", \"wb\") as training_file:\n",
    "            pkl.dump(self, training_file)\n",
    "        with open(f\"{output_folder}/game{suffix}.pkl\", \"wb\") as game_file:\n",
    "            pkl.dump(game, game_file)\n",
    "        with open(f\"{output_folder}/net{suffix}.pth\", \"wb\") as net_file:\n",
    "            torch.save(net.state_dict(), net_file)\n",
    "        with open(f\"{output_folder}/criterion{suffix}.pth\", \"wb\") as criterion_file:\n",
    "            torch.save(criterion.state_dict(), criterion_file)\n",
    "        with open(f\"{output_folder}/optimizer{suffix}.pth\", \"wb\") as optimizer_file:\n",
    "            torch.save(optimizer.state_dict(), optimizer_file)\n",
    "        # Saving the replay buffer via Pickle can lead to OOM, as Pickle creates a copy\n",
    "        # of the save in a VM\n",
    "        # What *should* be done is to not use Pickle at all, but as a stop-gap measure,\n",
    "        # I'm disabling replay buffer saving by default\n",
    "        if save_replay:\n",
    "            with open(f\"{output_folder}/replay{suffix}.pkl\", \"wb\") as replay_file:\n",
    "                pkl.dump(replay, replay_file)\n",
    "        \n",
    "    def epsilon_schedule(self, step, n_steps=250000, eps_max=1.0, eps_min=0.1):\n",
    "        \"\"\"\n",
    "        Linear anneal schedule\n",
    "\n",
    "        Taken from Mnih et al. 2013\n",
    "        \"\"\"\n",
    "        if (step < 1):\n",
    "            return eps_max\n",
    "        if (step > n_steps):\n",
    "            return eps_min\n",
    "        return (eps_min - eps_max) / (n_steps - 1) * (step - 1) + eps_max        \n",
    "\n",
    "    def train(self, game, net, criterion, optimizer, device, target_update_freq=10000, save_freq=500, replay=None, populate_states=12500, batch_size=32):\n",
    "        if replay is None:\n",
    "            replay = ReplayBuffer().populate(game, n_states=populate_states, n_frames_per_state=self.n_frames_per_state)\n",
    "        game.create()\n",
    "        \n",
    "        while (self.timestep < self.n_steps):\n",
    "            time_episode = time()\n",
    "            temp_time = time()\n",
    "            \n",
    "            reward_episode = 0\n",
    "            time_act = 0\n",
    "            time_replay = 0\n",
    "            time_batch_create = 0\n",
    "            time_batch_transfer = 0\n",
    "            time_qs = 0\n",
    "            time_train = 0\n",
    "            time_target_update = 0\n",
    "            \n",
    "            # Set up target network\n",
    "            target_net = model()\n",
    "            target_net.half()\n",
    "            target_net.to(device)\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "            \n",
    "            _ = game.reset()\n",
    "            # TODO: Should do a proper first step, not a random initialization\n",
    "            action = game.sample()\n",
    "            state, _, _, _ = game.step_state(action)\n",
    "    \n",
    "            time_init = time() - temp_time\n",
    "            for step in range(self.max_steps_per_eps):        \n",
    "                temp_time = time()\n",
    "                \n",
    "                # Epsilon scheduling\n",
    "                epsilon = self.epsilon_schedule(self.timestep)\n",
    "\n",
    "                # Generate action\n",
    "                q_values = net(torch.tensor(state.frames, dtype=torch.half, device=device).unsqueeze(0))\n",
    "                # Needs to reside on CPU to be fed to OpenAI Gym, and argmax doesn't accept half precision\n",
    "                q_values = q_values.clone().detach().float().cpu()\n",
    "                if np.random.random() < epsilon:\n",
    "                    action = game.sample()\n",
    "                else:\n",
    "                    # The typecast is needed to handle an edge case:\n",
    "                    # numpy() returns a 0D ndarray, which will cause the mini-batch\n",
    "                    # construction to throw an \"object does not have length\" error\n",
    "                    # if it's the first element in the mini-batch\n",
    "                    action = int(torch.argmax(q_values).data.numpy())\n",
    "\n",
    "                # Run environment and create next state\n",
    "                # To accelerate performance, we repeat the same action repeatedly\n",
    "                # for a fixed number of steps\n",
    "                next_state, reward, done, info = game.step_state(action)\n",
    "                reward_episode += reward\n",
    "                time_act += time() - temp_time\n",
    "                \n",
    "                temp_time = time()\n",
    "                replay.add(state, action, reward, next_state, done)\n",
    "                time_replay += time() - temp_time\n",
    "                \n",
    "                temp_time = time()\n",
    "                mini_batch = replay.create_mini_batch(batch_size=batch_size)\n",
    "                time_batch_create += time() - temp_time\n",
    "                \n",
    "                temp_time = time()\n",
    "                state_batch = mini_batch[\"state\"].to(device)\n",
    "                action_batch = mini_batch[\"action\"].to(device)\n",
    "                reward_batch = mini_batch[\"reward\"].to(device)\n",
    "                next_state_batch = mini_batch[\"next_state\"].to(device)\n",
    "                done_batch = mini_batch[\"done\"].to(device)\n",
    "                time_batch_transfer += time() - temp_time\n",
    "\n",
    "                temp_time = time()\n",
    "                # Get model predicted q values\n",
    "                cur_q = net(state_batch)\n",
    "                # Get target-model predicted q values\n",
    "                with torch.no_grad():\n",
    "                    # torch.max isn't implemented for half precision, so use single\n",
    "                    next_q = target_net(next_state_batch).float()\n",
    "\n",
    "                # Get the expected and target q-values\n",
    "                current_qval = cur_q.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
    "                target_qval = reward_batch + self.gamma * ((1 - done_batch) * torch.max(next_q, dim=1)[0])\n",
    "                # Convert back to half precision for optimizer             \n",
    "                target_qval = target_qval.half()\n",
    "                time_qs += time() - temp_time\n",
    "                \n",
    "                # Train model\n",
    "                temp_time = time()\n",
    "                loss = criterion(current_qval, target_qval.detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Perform gradient clipping:\n",
    "                for param in net.parameters():\n",
    "                    param.grad.clamp(-1, 1)\n",
    "                optimizer.step()\n",
    "                self.losses.append(loss.item())\n",
    "                time_train += time() - temp_time\n",
    "\n",
    "                temp_time = time()\n",
    "                if (self.timestep % target_update_freq == 0):\n",
    "                    target_net.load_state_dict(net.state_dict())\n",
    "                time_target_update += time() - temp_time\n",
    "                \n",
    "                self.timestep += 1\n",
    "\n",
    "                # Move to next iteration\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            self.rewards_episode.append(reward_episode)\n",
    "            time_episode = time() - time_episode\n",
    "            print(f\"Finished epoch {self.epoch}, total rewards {reward_episode}, total num steps {step}, epoch time {time_episode} s, final epsilon {epsilon}\")\n",
    "            print(f\"- init time {time_init} s\")\n",
    "            print(f\"- act time {time_act} s\")\n",
    "            print(f\"- replay time {time_replay} s\")\n",
    "            print(f\"- batch create time {time_batch_create} s\")\n",
    "            print(f\"- batch transfer time {time_batch_transfer} s\")\n",
    "            print(f\"- qs time {time_qs} s\")\n",
    "            print(f\"- train time {time_train} s\")\n",
    "            print(f\"- target update time {time_target_update} s\")\n",
    "            print(f\"- replay buffer length {len(replay.replay)}\")\n",
    "\n",
    "            if (self.output_folder is not None):\n",
    "                if self.epoch % save_freq == 0:\n",
    "                    time_save = time()\n",
    "                    self.save(game, net, criterion, optimizer, replay, suffix=f\".latest\")\n",
    "                    time_save = time() - time_save\n",
    "                    print(f\"Finished saving epoch {self.epoch}, total time to save {time_save} s\")\n",
    "                        \n",
    "            self.epoch += 1\n",
    "\n",
    "        self.save(game, net, criterion, optimizer, replay, suffix=f\".latest\")\n",
    "        \n",
    "        return self.losses, self.rewards_episode, replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training(output_folder, device, load_replay=False, suffix=\"\"):\n",
    "    net, criterion, optimizer = create_model(device)\n",
    "\n",
    "    with open(f\"{output_folder}/training{suffix}.pkl\", \"rb\") as training_file:\n",
    "        training = pkl.load(training_file)\n",
    "    with open(f\"{output_folder}/game{suffix}.pkl\", \"rb\") as game_file:\n",
    "        game = pkl.load(game_file)\n",
    "\n",
    "    with open(f\"{output_folder}/net{suffix}.pth\", \"rb\") as net_file:\n",
    "        net.load_state_dict(torch.load(net_file))\n",
    "        net.to(device)\n",
    "    with open(f\"{output_folder}/criterion{suffix}.pth\", \"rb\") as criterion_file:\n",
    "        criterion.load_state_dict(torch.load(criterion_file))\n",
    "    with open(f\"{output_folder}/optimizer{suffix}.pth\", \"rb\") as optimizer_file:\n",
    "        optimizer.load_state_dict(torch.load(optimizer_file))\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    replay = None\n",
    "    if load_replay:\n",
    "        with open(f\"{output_folder}/replay{suffix}.pkl\", \"rb\") as replay_file:\n",
    "            replay = pkl.load(replay_file)\n",
    "                    \n",
    "    return training, game, net, criterion, optimizer, replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.create()\n",
    "if MODE == \"start\":\n",
    "    net, criterion, optimizer = create_model(device)\n",
    "    replay = None\n",
    "    training = QLearning(output_folder=output_folder)\n",
    "elif MODE == \"load_from_disk\":\n",
    "    training, game, net, criterion, optimizer, replay = load_training(output_folder, device=device, suffix=\".latest\")\n",
    "losses, rewards_episode, replay = training.train(game, net, criterion, optimizer, replay=replay, device=device)\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from time import time\n",
    "from gym import wrappers\n",
    "\n",
    "def play_game(game, net, device, output_folder, n_steps=500, frame_rate=1./15.):\n",
    "    game.create()\n",
    "    _ = game.reset()\n",
    "    \n",
    "    # Pick a random action initially\n",
    "    print(\"1\")\n",
    "    action = game.sample()\n",
    "    state, _, _, _ = game.step_state(action)\n",
    "    reward_game = 0\n",
    "\n",
    "    while True:\n",
    "        print(\"2\")\n",
    "        q_values = net(torch.tensor(state.frames, dtype=torch.half, device=device).unsqueeze(0))\n",
    "        # Needs to reside on CPU to be fed to OpenAI Gym, and argmax doesn't accept half precision\n",
    "        with torch.no_grad():\n",
    "            q_values = net(torch.tensor(state.frames, dtype=torch.half, device=device).unsqueeze(0))\n",
    "            q_values = q_values.clone().detach().float().cpu()\n",
    "            action = int(torch.argmax(q_values).data.numpy())\n",
    "\n",
    "        print(\"3\")\n",
    "        next_state, reward, done, info = game.step_state(action)\n",
    "        game.env.render()\n",
    "        sleep(frame_rate)\n",
    "\n",
    "        print(\"4\")\n",
    "        reward_game += reward\n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        print(\"5\")\n",
    "        print(f\"Reward: {reward_game}\")\n",
    "        print(f\"Action: {action}\")\n",
    "        state = next_state\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, net, _, _, _ = load_training(output_folder, device, suffix=\".latest\")\n",
    "game = Game()\n",
    "play_game(game, net, device, \"videos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(\"videos/1588787664.302215/openaigym.video.1.7715.video000000.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
