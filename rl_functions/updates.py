"""Subroutines for updating non-class objects.

Currently, only the q function update is supported.

    Typical usage example:

    N/A
"""

import copy

def update_iterative(trajectory, rewards, q, alpha):
    """Update q function based on reward obtained from latest state-action pair.

    The update method used is a simple iterative approach, with scaling factor
    alpha.  If the state exists in the q-function but the state-action pair does
    not, the state-action pair will be initialized to the maximum q value for
    that state before applying the update.  If the state does not exist in the
    q-function, the state-action pair will be initialized to a value of zero
    before applying the update.

    Args:
        trajectory: The trajectory for the episode as a list of (state, action)
            pairs, i.e. [(state0, action0), (state1, action1), ...].
        rewards: The rewards generated by the environment for the trajectory
            as a list, i.e. [reward0, reward1, ...].  Sister array to
            trajectory.
        q: The q function represented as a two-level nested Iterable, i.e. of
            form q[state][action].  This argument is not updated in-place.
        alpha: The scaling factor for the update.

    Returns:
        The q function will be updated in-place.

    Raises:
        None
    """
    state, action = trajectory[-1]
    reward = rewards[-1]
    # if state does not exists in q, we insert it and the action into the
    # dictionary with a zero value
    if state not in q:
        q[state] = {}
        q[state][action] = 0.
    # if state exists in q but the action does not exist, we insert it into the
    # dictionary with the optimistic value
    if action not in q[state]:
        max_value = max(q[state].values())
        q[state][action] = max_value
    q_prev = q[state][action]
    q_new = q_prev + alpha * (reward - q_prev)
    q[state][action] = q_new

def update_on_policy_monte_carlo(trajectory, rewards, q, counts, gamma):
    """Update the q function using on-policy Monte Carlo control

    This update method is an on-policy Monte Carlo approach with discount factor
    gamma.  Only first-visit Monte Carlo is currently supported.  We store both
    the q-function and number of times a state-action pair has been visited to
    make calculating the new average G value easier and reduce memory overhead.
    If the state or state-action pair do not exist in the q-function, it will be
    added automatically.

    Args:
        trajectory: The trajectory for the episode as a list of (state, action)
            pairs, i.e. [(state0, action0), (state1, action1), ...].
        rewards: The rewards generated by the environment for the trajectory
            as a list, i.e. [reward0, reward1, ...].  Sister array to
            trajectory.
        q: The q function represented as a two-level nested Iterable, i.e. of
            form q[state][action].  This argument is not updated in-place.
        counts: The number of times a given state-action pair have been
            visited throughout the entire training process.  Sister array to
            q with the same structure (represented as a two-level nested
            Iterable, not updated in place).
        gamma: The discount factor for rewards.

    Returns:
        The q function and counts will be updated in-place.

    Raises:
        Exception when the length of rewards and trajectory is not the same.
    """
    if len(trajectory) != len(rewards):
        raise Exception(f"Trajectory and rewards have differing lengths of {len(trajectory)} and {len(rewards)}, respectively")
    traj = copy.deepcopy(trajectory)
    revs = copy.deepcopy(rewards)
    g = 0
    for time in range(len(traj)-1, -1, -1):
        state, action = traj.pop(time)
        reward = revs.pop(time)
        g = gamma * g + reward
        # Note: the following loop ensures that we only update q and counts
        # once for any given state-action pair based off of data that is
        # fixed (the current q/count value and the rewards).
        # If every visit is implemented, there is the possibility that partially
        # updated data will leak # downstream into the trajectoy unless we work
        # off copies of q and counts.  That being said, this leakage may
        # actually be a good thing.
        if (state, action) not in traj:
            if state not in q:
                q[state] = {}
                counts[state] = {}
            if action not in q[state]:
                q[state][action] = 0.0
                counts[state][action] = 0
            old_avg = q[state][action]
            old_count = counts[state][action]
            new_avg = (old_avg * old_count + g) / (old_count + 1)
            new_count = old_count + 1
            q[state][action] = new_avg
            counts[state][action] = new_count

def update_sarsa(trajectory, reward, q, alpha, gamma):
    """Update the q function using Sarsa control

    TODO:  This function breaks the pattern since the reward for the current
    time step is unknown.  Currently, I'm passing in the reward of the previous
    time step as a float.

    Args:
        trajectory: The trajectory for the episode as a list of (state, action)
            pairs, i.e. [(state0, action0), (state1, action1), ...].
        reward: The reward obtained from the previous time step.
        q: The q function represented as a two-level nested Iterable, i.e. of
            form q[state][action].  This argument is not updated in-place.
        alpha: The scaling factor for the update.
        gamma: The discount factor for rewards.

    Returns:
        The q function will be updated in-place.

    Raises:
        Exception when state-action pairs are not found in the q function.
    """
    s, a = trajectory[-2]
    if s not in q:
        raise Exception(f"previous state {s} not found in q function")
    if a not in q[s]:
        raise Exception(f"previous action {a} not found in q function")
    s_p, a_p = trajectory[-1]
    if s_p not in q:
        raise Exception(f"current state {s_p} not found in q function")
    if a_p not in q[s_p]:
        raise Exception(f"current action {a_p} not found in q function")
    # Calculate new q value for old (state, action) pair
    q_prev = q[s][a]
    q_next = q[s_p][a_p]
    q_prev = q_prev + alpha * (reward + gamma * q_next - q_prev)
    # Update q and return
    q[s][a] = q_prev
