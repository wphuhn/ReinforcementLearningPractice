"""Subroutines for updating non-class objects.

Currently, only the q function update is supported.

    Typical usage example:

    N/A
"""

import copy

class Control(object):
    """Core functionality for control classes
    """

    def __init__(self, q={}):
        """Initializes the core control functionality

        Args:
            q: (optional) An initial q-function to use, represented as a
                two-level nested Iterable, i.e. of form q[state][action].  If
                this argument is not supplied, an empty q-function will be
                initialized.

        Raises:
            None
        """
        self._q = copy.deepcopy(q)

    def get_q(self):
        """Returns the current q-function.

        Args:

        Returns:
            The q function represented as a two-level nested Iterable, i.e. of
            form q[state][action].

        Raises:
            None
        """
        return copy.deepcopy(self._q)

class IterativeControl(Control):
    """Control for updating q function based on current state-action reward

    Because this control has no memory, it is suitable for multi-armed bandits
    only.

    Attributes:
        alpha: The scaling factor for the update.
    """

    def __init__(self, alpha, q={}):
        """Initializes the iterative control

        Args:
            alpha: The scaling factor for the update.
            q: (optional) An initial q-function to use, represented as a
                two-level nested Iterable, i.e. of form q[state][action].  If
                this argument is not supplied, an empty q-function will be
                initialized.

        Raises:
            None
        """
        super().__init__(q=q)
        self.alpha = alpha

    def update(self, trajectory, rewards):
        """Update q function based on reward from latest state-action pair.

        The update method used is a simple iterative approach, with scaling
        factor alpha.  If the state exists in the q-function but the
        state-action pair does not, the state-action pair will be initialized to
        the maximum q value for that state before applying the update.  If the
        state does not exist in the q-function, the state-action pair will be
        initialized to a value of zero before applying the update.

        Args:
            trajectory: The trajectory for the episode as a list of
                (state, action) pairs, i.e. [(state0, action0),
                (state1, action1), ...].  Only the most recent element will be
                used by this algorithm.
            rewards: The rewards generated by the environment for the trajectory
                as a list, i.e. [reward0, reward1, ...].  Sister array to
                trajectory.  Only the most recent element will be used by this
                algorithm.

        Returns:
            None (updates the q-function internally).

        Raises:
            None
        """
        state, action = trajectory[-1]
        reward = rewards[-1]
        # if state does not exists in q, we insert it and the action into the
        # dictionary with a zero value
        if state not in self._q:
            self._q[state] = {}
            self._q[state][action] = 0.
        # if state exists in q but the action does not exist, we insert it into the
        # dictionary with the optimistic value
        if action not in self._q[state]:
            max_value = max(self._q[state].values())
            self._q[state][action] = max_value
        q_prev = self._q[state][action]
        q_new = q_prev + self.alpha * (reward - q_prev)
        self._q[state][action] = q_new

class OnPolicyMonteCarloControl(Control):
    def __init__(self, gamma, q={}, counts={}):
        super().__init__(q=q)
        self.gamma = gamma
        self._counts = copy.deepcopy(counts)

    def update(self, trajectory, rewards):
       """Update the q function using on-policy Monte Carlo control

       This update method is an on-policy Monte Carlo approach with discount factor
       gamma.  Only first-visit Monte Carlo is currently supported.  We store both
       the q-function and number of times a state-action pair has been visited to
       make calculating the new average G value easier and reduce memory overhead.
       If the state or state-action pair do not exist in the q-function, it will be
       added automatically.

       Args:
           trajectory: The trajectory for the episode as a list of (state, action)
               pairs, i.e. [(state0, action0), (state1, action1), ...].
           rewards: The rewards generated by the environment for the trajectory
               as a list, i.e. [reward0, reward1, ...].  Sister array to
               trajectory.
           q: The q function represented as a two-level nested Iterable, i.e. of
               form q[state][action].  This argument is not updated in-place.
           counts: The number of times a given state-action pair have been
               visited throughout the entire training process.  Sister array to
               q with the same structure (represented as a two-level nested
               Iterable, not updated in place).
           gamma: The discount factor for rewards.

       Returns:
           The q function and counts will be updated in-place.

       Raises:
           Exception when the length of rewards and trajectory is not the same.
       """
       if len(trajectory) != len(rewards):
           raise Exception(f"Trajectory and rewards have differing lengths of {len(trajectory)} and {len(rewards)}, respectively")
       traj = copy.deepcopy(trajectory)
       revs = copy.deepcopy(rewards)
       g = 0
       for time in range(len(traj)-1, -1, -1):
           state, action = traj.pop(time)
           reward = revs.pop(time)
           g = self.gamma * g + reward
           # Note: the following loop ensures that we only update q and counts
           # once for any given state-action pair based off of data that is
           # fixed (the current q/count value and the rewards).
           # If every visit is implemented, there is the possibility that partially
           # updated data will leak # downstream into the trajectoy unless we work
           # off copies of q and counts.  That being said, this leakage may
           # actually be a good thing.
           if (state, action) not in traj:
               if state not in self._q:
                   self._q[state] = {}
                   self._counts[state] = {}
               if action not in self._q[state]:
                   self._q[state][action] = 0.0
                   self._counts[state][action] = 0
               old_avg = self._q[state][action]
               old_count = self._counts[state][action]
               new_avg = (old_avg * old_count + g) / (old_count + 1)
               new_count = old_count + 1
               self._q[state][action] = new_avg
               self._counts[state][action] = new_count

    def get_counts(self):
        return copy.deepcopy(self._counts)

class SarsaControl(Control):
    def __init__(self, alpha, gamma, q={}):
        super().__init__(q=q)
        self.alpha = alpha
        self.gamma = gamma

    def update(self, trajectory, rewards):
        """Update the q function using Sarsa control

        Owing to the backup nature of this algorithm, depending on how one writes
        down the math for this algorithm the "current" reward could refer either to
        the reward from the previous time step (which is what enters into Sarsa) or
        the reward from the current time step.  To reduce confusion, I've decided to
        explicitly write the code in terms of the former definition.  This means
        that the rewards list should have one less element than the trajectory.

        Args:
            trajectory: The trajectory for the episode as a list of (state, action)
                pairs, i.e. [(state0, action0), (state1, action1), ...].
            rewards: The rewards generated by the environment for the trajectory
                as a list, i.e. [reward0, reward1, ...].  Sister array to
                trajectory.  Should not include the reward for the current time
                step!
            q: The q function represented as a two-level nested Iterable, i.e. of
                form q[state][action].  This argument is not updated in-place.
            alpha: The scaling factor for the update.
            gamma: The discount factor for rewards.

        Returns:
            The q function will be updated in-place.

        Raises:
            Exception when state-action pairs are not found in the q function.
        """
        if len(rewards) == len(trajectory):
            raise Exception("Length of trajectory and rewards lists are the same; current state-action pair shouldn't yet have a reward when doing Sarsa")
        reward = rewards[-1]

        s, a = trajectory[-2]
        if s not in self._q:
            raise Exception(f"previous state {s} not found in q function")
        if a not in self._q[s]:
            raise Exception(f"previous action {a} not found in q function")

        s_p, a_p = trajectory[-1]
        if s_p not in self._q:
            raise Exception(f"current state {s_p} not found in q function")
        if a_p not in self._q[s_p]:
            raise Exception(f"current action {a_p} not found in q function")

        # Calculate new q value for old (state, action) pair
        q_prev = self._q[s][a]
        q_next = self._q[s_p][a_p]
        q_prev = q_prev + self.alpha * (reward + self.gamma * q_next - q_prev)
        # Update q and return
        self._q[s][a] = q_prev

class QLearningControl(Control):
    def __init__(self, alpha, gamma, q={}):
        super().__init__(q=q)
        self.alpha = alpha
        self.gamma = gamma

    def update(self, trajectory, rewards):
        """Update the q function using Q-learning

        TODO

        Args:
            trajectory: The trajectory for the episode as a list of (state, action)
                pairs, i.e. [(state0, action0), (state1, action1), ...].  Note that
                the final "pair" should consist of only a state, i.e. (state, )
            rewards: The rewards generated by the environment for the trajectory
                as a list, i.e. [reward0, reward1, ...].  Sister array to
                trajectory.  Should not include the reward for the current time
                step!
            q: The q function represented as a two-level nested Iterable, i.e. of
                form q[state][action].  This argument is not updated in-place.
            alpha: The scaling factor for the update.
            gamma: The discount factor for rewards.

        Returns:
            The q function will be updated in-place.

        Raises:
            Exception when state-action pairs are not found in the q function.
        """
        if len(rewards) == len(trajectory):
            raise Exception("Length of trajectory and rewards lists are the same; current state shouldn't yet have a reward when doing Q-learning")
        if len(trajectory[-1]) > 1:
            raise Exception("current state shouldn't yet have an action when doing Q-learning")
        reward = rewards[-1]

        s, a = trajectory[-2]
        if s not in self._q:
            raise Exception(f"previous state {s} not found in q function")
        if a not in self._q[s]:
            raise Exception(f"previous action {a} not found in q function")

        s_p = trajectory[-1][0]
        if s_p not in self._q:
            raise Exception(f"current state {s_p} not found in q function")

        # Calculate new q value for old (state, action) pair
        q_prev = self._q[s][a]
        q_max = max(self._q[s_p].values())
        q_prev= q_prev + self.alpha * (reward + self.gamma * q_max - q_prev)
        # Update q and return
        self._q[s][a] = q_prev
